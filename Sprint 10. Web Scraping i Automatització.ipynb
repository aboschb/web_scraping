{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66869536",
   "metadata": {},
   "source": [
    " # Exercici 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c88d40",
   "metadata": {},
   "source": [
    "<b> Realitza web scraping de dues de les tres pàgines web proposades utilitzant BeautifulSoup primer i Selenium després. \n",
    "\n",
    "- http://quotes.toscrape.com\n",
    "\n",
    "- https://www.bolsamadrid.es\n",
    "\n",
    "- www.wikipedia.es (fes alguna cerca primer i escrapeja algun contingut) \n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfafd8",
   "metadata": {},
   "source": [
    "Després de la consulta dels 3 webs, hem decidit utilitzar el de Quotes i el de Wikipedia. \n",
    "Començarem fent web scraping de la pàgina http://quotes.toscrape.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6294d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Establim la URL de la pàgina web en la qual volem fer scraping\n",
    "url = \"http://quotes.toscrape.com\"\n",
    "\n",
    "# Fem una sol·licitud HTTP GET a la URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Trobem totes les cites, autors i etiquetes\n",
    "quotes = [quote.text.strip() for quote in soup.find_all('span', class_='text')]\n",
    "authors = [author.text.strip() for author in soup.find_all('small', class_='author')]\n",
    "tags = [[tag.text for tag in tag_div.find_all('a', class_='tag')] for tag_div in soup.find_all('div', class_='tags')]\n",
    "\n",
    "# Creem un DataFrame\n",
    "df_quotes = pd.DataFrame({\n",
    "    'Quote': quotes,\n",
    "    'Author': authors,\n",
    "    'Tags': tags\n",
    "})\n",
    "\n",
    "# Guardem el dataframe com a fitxer CSV\n",
    "df_quotes.to_csv('quotes_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57553d68",
   "metadata": {},
   "source": [
    "Com podem veure, hem obtingut cada cita amb 3 etiquetes: quote, author i tags, juntament amb el contingut de cadascuna.\n",
    "\n",
    "Ara anem a fer el mateix amb el web de wikipedia, concretament l'enllaç https://en.wikipedia.org/wiki/Wikipedia:Selected_anniversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a571f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Establim la URL de la pàgina web de la qual volem fer scraping\n",
    "url = \"https://en.wikipedia.org/wiki/Wikipedia:Selected_anniversaries\"\n",
    "\n",
    "# Fem una sol·licitud HTTP GET a la URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Llistes per emmagatzemar les dades\n",
    "years = []\n",
    "texts = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Analitzar el contingut HTML de la resposta amb BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extreiem cada element 'li' dins de la secció d'aniversaris seleccionats\n",
    "    list_items = soup.find_all('li')\n",
    "    for item in list_items:\n",
    "        year_link = item.find('a')\n",
    "        if year_link and year_link.get('title') and year_link.get('title').isdigit():\n",
    "            year = year_link.get('title').strip()  # Obtenir l'any\n",
    "            text = item.get_text(strip=True).replace(year, '', 1).strip()  # Eliminar l'any del text\n",
    "            \n",
    "            # Afegir les dades a les llistes\n",
    "            years.append(year)\n",
    "            texts.append(text)\n",
    "else:\n",
    "    print(\"Error en la sol·licitud: Status Code\", response.status_code)\n",
    "\n",
    "# Crear un DataFrame amb les dades extretes\n",
    "df_anniversaries = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'Text': texts\n",
    "})\n",
    "\n",
    "# Guardar el DataFrame com a fitxer CSV\n",
    "df_anniversaries.to_csv('anniversaries_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181aee4",
   "metadata": {},
   "source": [
    "Com podem veure, hem aconseguit extreure amb èxit informació específica de la pàgina \"Selected Anniversaries\" de Wikipedia. Per a cada entrada d'aniversari, hem obtingut l'any, indicat per l'atribut title de l'element d'enllaç (a), i el text descriptiu associat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82e58b",
   "metadata": {},
   "source": [
    "Ara farem web scraping del primer web mitjançant Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0d5b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "Author: Albert Einstein\n",
      "Tags: ['change', 'deep-thoughts', 'thinking', 'world']\n",
      "\n",
      "Quote: “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "Author: J.K. Rowling\n",
      "Tags: ['abilities', 'choices']\n",
      "\n",
      "Quote: “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "Author: Albert Einstein\n",
      "Tags: ['inspirational', 'life', 'live', 'miracle', 'miracles']\n",
      "\n",
      "Quote: “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "Author: Jane Austen\n",
      "Tags: ['aliteracy', 'books', 'classic', 'humor']\n",
      "\n",
      "Quote: “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "Author: Marilyn Monroe\n",
      "Tags: ['be-yourself', 'inspirational']\n",
      "\n",
      "Quote: “Try not to become a man of success. Rather become a man of value.”\n",
      "Author: Albert Einstein\n",
      "Tags: ['adulthood', 'success', 'value']\n",
      "\n",
      "Quote: “It is better to be hated for what you are than to be loved for what you are not.”\n",
      "Author: André Gide\n",
      "Tags: ['life', 'love']\n",
      "\n",
      "Quote: “I have not failed. I've just found 10,000 ways that won't work.”\n",
      "Author: Thomas A. Edison\n",
      "Tags: ['edison', 'failure', 'inspirational', 'paraphrased']\n",
      "\n",
      "Quote: “A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "Author: Eleanor Roosevelt\n",
      "Tags: ['misattributed-eleanor-roosevelt']\n",
      "\n",
      "Quote: “A day without sunshine is like, you know, night.”\n",
      "Author: Steve Martin\n",
      "Tags: ['humor', 'obvious', 'simile']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Especifiquem la ruta del ChromeDriver\n",
    "PATH_TO_WEBDRIVER = \"C:/Users/ariad/Desktop/chromedriver-win64/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# Inicialitzem el servei ChromeDriver\n",
    "service = Service(executable_path=PATH_TO_WEBDRIVER)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Obrim la pàgina web\n",
    "driver.get(\"http://quotes.toscrape.com\")\n",
    "\n",
    "# Esperem que la pàgina carregui completament\n",
    "time.sleep(2)\n",
    "\n",
    "# Extreiem les cites, autors i etiquetes\n",
    "quotes = driver.find_elements(By.CSS_SELECTOR, \"span.text\")\n",
    "authors = driver.find_elements(By.CSS_SELECTOR, \"small.author\")\n",
    "tags = [tag.find_elements(By.CSS_SELECTOR, \"a.tag\") for tag in driver.find_elements(By.CSS_SELECTOR, \"div.tags\")]\n",
    "\n",
    "for i in range(len(quotes)):\n",
    "    print(\"Quote:\", quotes[i].text)\n",
    "    print(\"Author:\", authors[i].text)\n",
    "    print(\"Tags:\", [tag.text for tag in tags[i]])\n",
    "    print()\n",
    "\n",
    "# Tanquem el navegador un cop acabat\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fe739",
   "metadata": {},
   "source": [
    "Com podem veure, el codi ha aconseguit navegar automàticament a través del navegador, accedir a la pàgina web especificada i extreure dades específiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba45923",
   "metadata": {},
   "source": [
    "Ara farem web scraping del segon web mitjançant Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d1deb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 1776\n",
      "Text: Holidays/observances should be limited to a maximum of 3, but this may increase to accommodate those that do not appear on the same date each year. Holidays should be listed in this order: international observances (secular followed by religious) first (except those that are not serious in nature, e.g., International Talk Like a Pirate Day—these should appear at the end of the list), then alphabetically by country where observed. When the same day is observed in multiple countries (but not worldwide), then the countries should be listed alphabetically, and the first country used for sorting.\n",
      "Example (note that these are not actually all on the same date, but combined to demonstrate the sorting): Earth Day (2024); Easter (Christianity, 2024); Mother's Day in Belize and Guatemala; Independence Day in the United States (); Star Wars Day\n",
      "Year: 1776\n",
      "Text: Example (note that these are not actually all on the same date, but combined to demonstrate the sorting): Earth Day (2024); Easter (Christianity, 2024); Mother's Day in Belize and Guatemala; Independence Day in the United States (); Star Wars Day\n",
      "Year: 1749\n",
      "Text: – The first issue of Berlingske (front page pictured), Denmark's oldest continually operating newspaper, was published.\n",
      "Year: 1911\n",
      "Text: – An earthquake registering 7.7 Mw destroyed Almaty in Russian Turkestan.\n",
      "Year: 1938\n",
      "Text: – The American health charity March of Dimes was founded as the National Foundation for Infantile Paralysis to help raise money for polio research.\n",
      "Year: 1961\n",
      "Text: – All 25 people on board Aero Flight 311 died in Finland's worst civilian air accident when the aircraft crashed near Kvevlax.\n",
      "Year: 2009\n",
      "Text: – The cryptocurrency network of bitcoin was created when Satoshi Nakamoto mined the first block of the chain.\n",
      "Year: 1698\n",
      "Text: – Most of London's Palace of Whitehall, the main residence of English monarchs since 1530, was destroyed by fire.\n",
      "Year: 1798\n",
      "Text: – After his appointment as Prince of Wallachia, Constantine Hangerli arrived in Bucharest to assume the throne.\n",
      "Year: 1936\n",
      "Text: – Billboard published its first music hit parade.\n",
      "Year: 1989\n",
      "Text: – Two American F-14 Tomcats shot down two Libyan MiG-23 Floggers that appeared to be attempting to engage them over the Gulf of Sidra.\n",
      "Year: 2004\n",
      "Text: – Spirit (artist's impression pictured), the first of two rovers of NASA's Mars Exploration Rover mission, successfully landed on Mars.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Especifiquem la ruta del ChromeDriver\n",
    "PATH_TO_WEBDRIVER = \"C:/Users/ariad/Desktop/chromedriver-win64/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# Inicialitzem el servei ChromeDriver\n",
    "service = Service(executable_path=PATH_TO_WEBDRIVER)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Obrim la pàgina web de Wikipedia: Selected Anniversaries\n",
    "driver.get(\"https://en.wikipedia.org/wiki/Wikipedia:Selected_anniversaries\")\n",
    "\n",
    "# Esperem que la pàgina carregui completament\n",
    "time.sleep(2)\n",
    "\n",
    "# Extreiem els anniversaris\n",
    "anniversaries = driver.find_elements(By.CSS_SELECTOR, \"ul li\")\n",
    "for item in anniversaries:\n",
    "    links = item.find_elements(By.CSS_SELECTOR, \"a\")\n",
    "    if links:\n",
    "        for link in links:\n",
    "            if link.get_attribute('title') and link.get_attribute('title').isdigit():\n",
    "                year = link.get_attribute('title').strip()\n",
    "                text = item.text.replace(year, '', 1).strip()\n",
    "                print(\"Year:\", year)\n",
    "                print(\"Text:\", text)\n",
    "                break  # Només considerem el primer enllaç amb un any\n",
    "\n",
    "# Tanquem el navegador un cop acabat\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bb07c",
   "metadata": {},
   "source": [
    "Com podem veure, hem aconseguit extreure amb èxit la informació dels aniversaris seleccionats de la pàgina de Wikipedia. Això ens proporciona una visió detallada i organitzada dels esdeveniments històrics i les celebracions associades a cada any."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b63317",
   "metadata": {},
   "source": [
    "# Exercici 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8d5af",
   "metadata": {},
   "source": [
    "<b> Documenta en un Word el teu conjunt de dades generat amb la informació que tenen els diferents arxius de Kaggle.\n",
    "A manera d'exemple del que es demana pots consultar aquest enllaç:\n",
    "->https://www.kaggle.com/datasets/vivovinco/20212022-football-team-stats. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cf44b",
   "metadata": {},
   "source": [
    "En primer lloc visualitzem els dataframes obtinguts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711033da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The world as we have created it is a process ...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>[change, deep-thoughts, thinking, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“It is our choices, Harry, that show what we t...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>[abilities, choices]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“There are only two ways to live your life. On...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>[inspirational, life, live, miracle, miracles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The person, be it gentleman or lady, who has ...</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>[aliteracy, books, classic, humor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Imperfection is beauty, madness is genius and...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>[be-yourself, inspirational]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Quote           Author  \\\n",
       "0  “The world as we have created it is a process ...  Albert Einstein   \n",
       "1  “It is our choices, Harry, that show what we t...     J.K. Rowling   \n",
       "2  “There are only two ways to live your life. On...  Albert Einstein   \n",
       "3  “The person, be it gentleman or lady, who has ...      Jane Austen   \n",
       "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
       "\n",
       "                                             Tags  \n",
       "0        [change, deep-thoughts, thinking, world]  \n",
       "1                            [abilities, choices]  \n",
       "2  [inspirational, life, live, miracle, miracles]  \n",
       "3              [aliteracy, books, classic, humor]  \n",
       "4                    [be-yourself, inspirational]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quotes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "061a47b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1749</td>\n",
       "      <td>– The first issue ofBerlingske(front page pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1911</td>\n",
       "      <td>–An earthquakeregistering 7.7MwdestroyedAlmaty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1938</td>\n",
       "      <td>– The American health charityMarch of Dimeswas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1961</td>\n",
       "      <td>– All 25 people on boardAero Flight 311died in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>– Thecryptocurrencynetwork ofbitcoinwas create...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year                                               Text\n",
       "0  1749  – The first issue ofBerlingske(front page pict...\n",
       "1  1911  –An earthquakeregistering 7.7MwdestroyedAlmaty...\n",
       "2  1938  – The American health charityMarch of Dimeswas...\n",
       "3  1961  – All 25 people on boardAero Flight 311died in...\n",
       "4  2009  – Thecryptocurrencynetwork ofbitcoinwas create..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anniversaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58e198",
   "metadata": {},
   "source": [
    "Mirarem també el tamany de cada dataset (files i columnes) per documentar-ho després."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5ac0156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quotes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9ce273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anniversaries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0a4ae",
   "metadata": {},
   "source": [
    "També podem mirar el llistat d'autors del dataframe quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b4a4769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert Einstein',\n",
       " 'J.K. Rowling',\n",
       " 'Jane Austen',\n",
       " 'Marilyn Monroe',\n",
       " 'André Gide',\n",
       " 'Thomas A. Edison',\n",
       " 'Eleanor Roosevelt',\n",
       " 'Steve Martin']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_quotes['Author'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48e870",
   "metadata": {},
   "source": [
    "I el tipus de tags o etiquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e39dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['change' 'deep-thoughts' 'thinking' 'world' 'abilities' 'choices'\n",
      " 'inspirational' 'life' 'live' 'miracle' 'miracles' 'aliteracy' 'books'\n",
      " 'classic' 'humor' 'be-yourself' 'adulthood' 'success' 'value' 'love'\n",
      " 'edison' 'failure' 'paraphrased' 'misattributed-eleanor-roosevelt'\n",
      " 'obvious' 'simile']\n",
      "Tags\n",
      "inspirational                      3\n",
      "humor                              2\n",
      "life                               2\n",
      "change                             1\n",
      "obvious                            1\n",
      "misattributed-eleanor-roosevelt    1\n",
      "paraphrased                        1\n",
      "failure                            1\n",
      "edison                             1\n",
      "love                               1\n",
      "value                              1\n",
      "success                            1\n",
      "adulthood                          1\n",
      "be-yourself                        1\n",
      "classic                            1\n",
      "deep-thoughts                      1\n",
      "books                              1\n",
      "aliteracy                          1\n",
      "miracles                           1\n",
      "miracle                            1\n",
      "live                               1\n",
      "choices                            1\n",
      "abilities                          1\n",
      "world                              1\n",
      "thinking                           1\n",
      "simile                             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Utilitzem explode per transformar cada tag en una fila separada\n",
    "df_tags_exploded = df_quotes.explode('Tags')\n",
    "\n",
    "# Ara podem veure els tags únics fent servir unique() o value_counts() per veure la freqüència\n",
    "unique_tags = df_tags_exploded['Tags'].unique()\n",
    "print(unique_tags)\n",
    "\n",
    "# O per veure la freqüència de cada tag\n",
    "tag_counts = df_tags_exploded['Tags'].value_counts()\n",
    "print(tag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e072e3",
   "metadata": {},
   "source": [
    "Ara, del dataframe sobre aniversaris, podem visualitzar els diferents anys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "098f2121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1749',\n",
       " '1911',\n",
       " '1938',\n",
       " '1961',\n",
       " '2009',\n",
       " '1698',\n",
       " '1798',\n",
       " '1936',\n",
       " '1989',\n",
       " '2004']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_anniversaries['Year'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ce96b",
   "metadata": {},
   "source": [
    "A més, hem observat els diferents esdeveniments per classificar-los per temàtiques. La mida limitada del dataset ens ha permès fer-ho així, a partir de la simple lectura de les dades. \n",
    "\n",
    "A partir d'aquesta informació, i seguint l'enllaç recomananat com a guia -a més d'altres datasets de Kaggle- s'ha creat el document word/pdf corresponent (consultar-lo en aquest repositori)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e9050",
   "metadata": {},
   "source": [
    "# Exercici 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c559b2",
   "metadata": {},
   "source": [
    "<b>Tria una pàgina web que tu vulguis i realitza web scraping mitjançant la llibreria Selenium primer i Scrapy després. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5cb3e",
   "metadata": {},
   "source": [
    "Hem escollit la pàgina web de Goodreads, concretament la secció de llibres sobre Data Science: <br>\n",
    "https://www.goodreads.com/search?q=data+science&qid=\n",
    "\n",
    "Comencem fent web scraping amb Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb84ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Títol: Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking\n",
      "Autor: Foster Provost, Tom Fawcett\n",
      "-------------------------------------------------\n",
      "Títol: Data Smart: Using Data Science to Transform Information into Insight\n",
      "Autor: John W. Foreman\n",
      "-------------------------------------------------\n",
      "Títol: Data Science from Scratch: First Principles with Python\n",
      "Autor: Joel Grus\n",
      "-------------------------------------------------\n",
      "Títol: Machine Learning For Absolute Beginners: A Plain English Introduction (Second Edition)\n",
      "Autor: Oliver Theobald\n",
      "-------------------------------------------------\n",
      "Títol: Numsense! Data Science for the Layman: No Math Added\n",
      "Autor: Annalyn Ng, Kenneth Soo\n",
      "-------------------------------------------------\n",
      "Títol: Doing Data Science: Straight Talk from the Frontline\n",
      "Autor: Cathy O'Neil, Rachel Schutt\n",
      "-------------------------------------------------\n",
      "Títol: Python Data Science Handbook: Essential Tools for Working with Data\n",
      "Autor: Jake Vanderplas\n",
      "-------------------------------------------------\n",
      "Títol: Data Science (The MIT Press Essential Knowledge series)\n",
      "Autor: John D. Kelleher, Brendan Tierney\n",
      "-------------------------------------------------\n",
      "Títol: Data Science For Dummies (For Dummies\n",
      "Autor: Lillian Pierson\n",
      "-------------------------------------------------\n",
      "Títol: The Art of Data Science: A Guide for Anyone Who Works with Data\n",
      "Autor: Roger D. Peng, Elizabeth Matsui\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Importem les llibreries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Configurem la ruta on es troba el chromedriver\n",
    "PATH_TO_WEBDRIVER = \"C:/Users/ariad/Desktop/chromedriver-win64/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# Inicialitzem el webdriver amb la classe Service\n",
    "service = Service(executable_path=PATH_TO_WEBDRIVER)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Obrim la pàgina web\n",
    "driver.get(\"https://www.goodreads.com/search?q=data+science&qid=\")\n",
    "\n",
    "# Esperem a que la pàgina carregui completament\n",
    "time.sleep(5)\n",
    "\n",
    "# Trobem tots els blocs de llibres\n",
    "book_blocks = driver.find_elements(By.CSS_SELECTOR, \"tr[itemtype='http://schema.org/Book']\")\n",
    "\n",
    "# Processem cada bloc per separar títols i autors\n",
    "books = []\n",
    "for block in book_blocks:\n",
    "    title_element = block.find_element(By.CSS_SELECTOR, \"span[itemprop='name']\")\n",
    "    title = title_element.text.strip() if title_element else \"Títol desconegut\"\n",
    "\n",
    "    # Intentar trobar l'element que conté el text complet dels autors\n",
    "    author_elements = block.find_elements(By.XPATH, \".//div[@class='authorName__container']/a[@class='authorName']\")\n",
    "    authors = ', '.join([author.text.strip() for author in author_elements]) if author_elements else \"Autor desconegut\"\n",
    "\n",
    "    books.append({\"title\": title, \"author\": authors})\n",
    "\n",
    "# Mostrem els resultats\n",
    "for book in books:\n",
    "    print(f\"Títol: {book['title']}\")\n",
    "    print(f\"Autor: {book['author']}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "# Tanquem el navegador un cop acabat\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663bbf5",
   "metadata": {},
   "source": [
    "Ara farem scraping de la pàgina web:\n",
    "https://www.bbcgoodfood.com/recipes/collection/vegan-recipes\n",
    "amb l'objectiu d'extreure una llista de receptes veganes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9173c",
   "metadata": {},
   "source": [
    "Primer de tot instal.lem Scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0daeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install scrapy -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6297a33",
   "metadata": {},
   "source": [
    "Ara importem les llibreries necessàries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ee532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f854d2",
   "metadata": {},
   "source": [
    "Ara creem un nou projecte de Scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a8c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'bbcgoodfood_scraper', using template directory 'C:\\Users\\ariad\\anaconda3\\Lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Users\\ariad\\Desktop\\IT ACADEMY\\ESPECIALITZACIÓ\\SPRINT 10\\bbcgoodfood_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd bbcgoodfood_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject bbcgoodfood_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17b057",
   "metadata": {},
   "source": [
    "Ara definim l'Spider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91182c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBCGoodFoodSpider(scrapy.Spider):\n",
    "    name = 'bbcgoodfood'\n",
    "    start_urls = ['https://www.bbcgoodfood.com/recipes/collection/vegan-recipes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for recipe in response.css('h2.heading-4'):\n",
    "            yield {\n",
    "                'title': recipe.css('::text').get(),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037a7fa",
   "metadata": {},
   "source": [
    "Aquest codi ha creat un spider anomenat bbcgoodfood que comença a extreure dades de l'URL proporcionada. Busca tots els elements h2 amb la classe heading-4, que corresponen als títols de les receptes en la pàgina i extreu el text d'aquests elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542219f",
   "metadata": {},
   "source": [
    "Configurem l'arxiu settings.py (es troba dins de la carpeta creada pel projecte) indicant que ignori la restricció en l'ús de robots, és a dir canviant el valor de True a False:\n",
    "ROBOTSTXT_OBEY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585af22",
   "metadata": {},
   "source": [
    "Ara fem l'scraping de la pàgina web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9390721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 11:10:38 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2024-01-22 11:10:38 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.2.0 23 Nov 2023), cryptography 41.0.7, Platform Windows-10-10.0.22621-SP0\n",
      "2024-01-22 11:10:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
      "2024-01-22 11:10:38 [py.warnings] WARNING: C:\\Users\\ariad\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-01-22 11:10:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-01-22 11:10:39 [scrapy.extensions.telnet] INFO: Telnet Password: 492c241d1d5f2b21\n",
      "2024-01-22 11:10:39 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-01-22 11:10:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-01-22 11:10:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-01-22 11:10:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-01-22 11:10:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-01-22 11:10:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-01-22 11:10:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-01-22 11:10:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.bbcgoodfood.com/recipes/collection/vegan-recipes> (referer: None)\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Sweet potato & peanut curry'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan jambalaya'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Easy vegan wellington'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan chilli'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Cranberry, orange & rosemary sauce'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Simple sweet & sour slow-cooker red cabbage'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan Christmas pudding'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Crispy banana fritters (pisang goreng wijen)'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Slow-cooker pumpkin soup'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan simnel cake'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan mayonnaise'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'No-bake flapjacks'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan chickpea curry jacket potatoes'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Easy vegan pancakes'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan scones'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Speedy lentil coconut curry'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Hasselback potatoes'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan burritos'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan banana muffins'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Roasted aubergine & tomato curry'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan sausage rolls'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'App only'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan chocolate party traybake'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Vegan trifle'}\n",
      "2024-01-22 11:10:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.bbcgoodfood.com/recipes/collection/vegan-recipes>\n",
      "{'title': 'Christmas Subscription Offer'}\n",
      "2024-01-22 11:10:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-01-22 11:10:40 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: recipes.json\n",
      "2024-01-22 11:10:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 341,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 82835,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.359656,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 1, 22, 10, 10, 40, 242992),\n",
      " 'httpcompression/response_bytes': 641584,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/DEBUG': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 1, 22, 10, 10, 39, 883336)}\n",
      "2024-01-22 11:10:40 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class BBCGoodFoodSpider(scrapy.Spider):\n",
    "    name = 'bbcgoodfood'\n",
    "    start_urls = ['https://www.bbcgoodfood.com/recipes/collection/vegan-recipes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for recipe in response.css('h2.heading-4'):\n",
    "            yield {\n",
    "                'title': recipe.css('::text').get(),\n",
    "            }\n",
    "\n",
    "# Configuració del procés de rastreig amb la sortida a un arxiu JSON\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEEDS': {\n",
    "        'recipes.json': {\n",
    "            'format': 'json',\n",
    "            'encoding': 'utf8',\n",
    "        },\n",
    "    },\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "})\n",
    "\n",
    "# Iniciar el procés de rastreig\n",
    "process.crawl(BBCGoodFoodSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22d588",
   "metadata": {},
   "source": [
    "Finalment, visualitzem els resultats de l'arxiu recipes.json creat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7105342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Sweet potato & peanut curry'}\n",
      "{'title': 'Vegan jambalaya'}\n",
      "{'title': 'Easy vegan wellington'}\n",
      "{'title': 'Vegan chilli'}\n",
      "{'title': 'Cranberry, orange & rosemary sauce'}\n",
      "{'title': 'Simple sweet & sour slow-cooker red cabbage'}\n",
      "{'title': 'Vegan Christmas pudding'}\n",
      "{'title': 'Crispy banana fritters (pisang goreng wijen)'}\n",
      "{'title': 'Slow-cooker pumpkin soup'}\n",
      "{'title': 'Vegan simnel cake'}\n",
      "{'title': 'Vegan mayonnaise'}\n",
      "{'title': 'No-bake flapjacks'}\n",
      "{'title': 'Vegan chickpea curry jacket potatoes'}\n",
      "{'title': 'Easy vegan pancakes'}\n",
      "{'title': 'Vegan scones'}\n",
      "{'title': 'Speedy lentil coconut curry'}\n",
      "{'title': 'Hasselback potatoes'}\n",
      "{'title': 'Vegan burritos'}\n",
      "{'title': 'Vegan banana muffins'}\n",
      "{'title': 'Roasted aubergine & tomato curry'}\n",
      "{'title': 'Vegan sausage rolls'}\n",
      "{'title': 'App only'}\n",
      "{'title': 'Vegan chocolate party traybake'}\n",
      "{'title': 'Vegan trifle'}\n",
      "{'title': 'Christmas Subscription Offer'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Defineix la ruta de l'arxiu JSON\n",
    "file_path = 'recipes.json'\n",
    "\n",
    "# Obrir l'arxiu JSON i llegir les dades\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Mostrar les dades\n",
    "for recipe in data:\n",
    "    print(recipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a1c2e",
   "metadata": {},
   "source": [
    "Com podem veure, se'ns ha creat una llista amb els noms de les receptes de la pàgina web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
